
```{r}
# Load function used
library(VIM)
library(JointAI)
library(tidyverse)
library(ggpubr)
library(glmnet)
library(Metrics)
library(kableExtra)
```


```{r}
# Inputs 
xtrain <- readRDS("geo_methylation.rds")
xtrain <- as.data.frame(xtrain)
ytrain <- readRDS("geo_samples.rds")
# Sanity check 
identical(rownames(xtrain), rownames(ytrain))
```

------------
Data Pattern
------------

```{r}
# Check missing of xtrain
aggr(xtrain,numbers=TRUE)
# Check missing of ytrain
md_pattern(ytrain, pattern = FALSE, color = c('cornsilk', 'blue4'))
# summarize missing of ytrain
ytrain %>% filter(is.na(Sex)) %>% group_by(GEO) %>% count()
```

-----------------
Data information
-----------------

```{r}
# Obtain number of samples, mean and standard deviation of age for each group
tab1 <- ytrain %>% group_by(GEO) %>% summarise(N=n(), Mean=round(mean(Age),1),Sd=round(sd(Age),1))
# Obtain number of males and females for each group
tab2 <- ytrain %>% group_by(GEO,Sex) %>% summarise(N=n()) %>% pivot_wider(names_from = Sex, values_from = N) %>% rename(Male=M,Female=F) %>% dplyr::select(-"NA")
# Combine 2 data frames
tab <- left_join(tab1,tab2,by="GEO")
tab <- as.data.frame(tab)
colnames(tab)[1] <- "Cohort"
# Add sample type for each group
tab[,"Type"] <- c(rep("Blood",7),"Saliva")
# Calculate the information of the whole data
tab[nrow(tab)+1,] <- c("Total",nrow(ytrain),round(mean(ytrain$Age),1),round(sd(ytrain$Age),1),nrow(ytrain[which(ytrain$Sex=="F"),]),nrow(ytrain[which(ytrain$Sex=="M"),]),"-")
# Polish table
tab <- ggtexttable(tab, rows = NULL, theme = ttheme("lCyanWhite",tbody.style =tbody_style(color = "black",fill="white")))
tab <- tab %>%
 table_cell_font(row =2:tab_nrow(tab), column = 1:tab_ncol(tab), size = 10)  %>%
 table_cell_bg(row =5, column = c(1,2,5,6), fill = "darkblue", linewidth = 0) %>%
 table_cell_font(row = 5, column = c(1,2,5,6), color = "white", size = 10)
tab
```

----------------
Age Distribution
----------------

```{r}
# Draw histograms of age distributions for each group
p1 <- ggplot(data = ytrain, mapping = aes(x = Age)) + geom_histogram(aes(y = ..density..,fill=GEO),bins = 20,col="black",size=0.4) + facet_wrap(~GEO, ncol = 4) + scale_fill_brewer(palette = "Spectral") + theme(legend.position = "none")
# Draw a histogram of age distributions for entire data
p2 <- ggplot(data = ytrain, mapping = aes(x = Age)) + geom_histogram(aes(y = ..density..),bins = 30, col="black",fill="cornflowerblue") + geom_density(color='aliceblue', size=1)
# Plot 2 figures together
ggarrange(p1, p2, ncol=1, nrow=2) 
```

---------
Sex Ratio
---------

```{r}
# Summarise the number of males and females for each group
sex <- ytrain %>% group_by(GEO,Sex) %>% summarise(N=n()) %>% 
  pivot_wider(names_from = Sex, values_from = N) %>% 
  rename(Male=M,Female=F) %>% mutate(`NA` = replace_na(`NA`, 0)) %>%
  pivot_longer(!GEO, names_to = "Sex", values_to = "Count")
# Calculate the ratio of males and females for each group
sex <- sex %>% group_by(GEO) %>% mutate(prop = Count/ sum(Count)) %>%
  mutate(label = scales::percent(prop))
# Count the number and calculate the ratio of males and females for the whole data
sextotal <- ytrain %>% group_by(Sex) %>% summarise(N=n()) %>% 
  mutate(prop = N/ sum(N)) %>% mutate(label = scales::percent(prop)) %>% mutate(Sex = replace(Sex, Sex=="M", "Male"), Sex=replace(Sex, Sex=="F", "Female"),Sex=replace(Sex, is.na(Sex), "NA"))

# Draw pie charts of gender for each group
p1 <- ggplot(sex, aes(x="", y=prop, fill=Sex)) + geom_col() + 
  coord_polar(theta = "y") + theme_void() + 
  geom_text(aes(label = label),
            position = position_stack(vjust = 0.5),size=3.2) +
  scale_fill_brewer(palette="Blues") + facet_wrap(~GEO,ncol=4) 
# Draw a pie chart of gender for the whole data
p2 <- ggplot(sextotal, aes(x="", y=prop, fill=Sex)) + geom_col() + 
  coord_polar(theta = "y") + theme_void() + 
  geom_text(aes(label = label),
            position = position_stack(vjust = 0.5),size=4) + 
    scale_fill_brewer(palette="Blues") + ggtitle("Total") + 
  theme(plot.title = element_text(hjust = 0.5))
# Plot 2 figures together
ggarrange(p1, p2, ncol=2, nrow=1, widths = c(1.5, 0.5),common.legend = TRUE,legend = "bottom") 
```

--------------------------------
Relationship between age and sex
--------------------------------

```{r}
# Draw boxplots of the relationship between age and sex for each group
p1 <- ggplot(data = subset(ytrain,!is.na(Sex)), mapping = aes(x =Age, y=Sex,fill=Sex)) + geom_boxplot() + facet_wrap(~GEO, ncol = 4) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set2")
# Draw a boxplot of the relationship between age and sex for the whole data
p2 <- ggplot(data = subset(ytrain,!is.na(Sex)), mapping = aes(x = Age, y=Sex,fill=Sex)) + geom_boxplot() + theme(legend.position = "none") + scale_fill_brewer(palette = "Set3")
# Plot 2 figures together
ggarrange(p1, p2, ncol=1, nrow=2) 
```

--------------------------------------
Compare 3 penalized regression methods
--------------------------------------

```{r}
xtrain <- as.matrix(xtrain)
set.seed(1)
# Cross validation to get best lambda
ridge.cv <- cv.glmnet(xtrain, ytrain$Age, family="gaussian", alpha = 0, nfolds=10)
elnet.cv <- cv.glmnet(xtrain, ytrain$Age, family="gaussian", alpha = 0.5, nfolds=10)
lasso.cv <- cv.glmnet(xtrain, ytrain$Age, family="gaussian", alpha = 1, nfolds=10)
```


```{r}
# Store ytrain for 3 models
ytrain_ridge <- ytrain
ytrain_elnet <- ytrain
ytrain_lasso <- ytrain
# Apply leave one out with 3 models and store the predicted ages
for (i in unique(ytrain$GEO)){
  onenum <- which(ytrain$GEO==i)
  restnum <- which(ytrain$GEO!=i)
  # Now get best model with defined parameters
  ridge.fit <- glmnet(xtrain[restnum,], ytrain[restnum,"Age"], family = "gaussian",
                      alpha = 0, lambda = ridge.cv$lambda.min)
  elnet.fit <- glmnet(xtrain[restnum,], ytrain[restnum,"Age"], family = "gaussian",
                      alpha = 0.5, lambda = elnet.cv$lambda.min)
  lasso.fit <- glmnet(xtrain[restnum,], ytrain[restnum,"Age"], family = "gaussian",
                      alpha = 1, lambda = lasso.cv$lambda.min)
  ytrain_ridge[onenum,"PreAge"] <- predict(ridge.fit, newx = xtrain[onenum,], type = "response", s = "lambda.min")
  ytrain_elnet[onenum,"PreAge"] <- predict(elnet.fit, newx = xtrain[onenum,], type = "response", s = "lambda.min")
  ytrain_lasso[onenum,"PreAge"] <- predict(lasso.fit, newx = xtrain[onenum,], type = "response", s = "lambda.min")
}
```


```{r}
# Calculate RMSE and MAE for the whole data for 3 models 
# Make table to display all the results
df <- data.frame(RMSE=c(rmse(ytrain_ridge$Age, ytrain_ridge$PreAge), 
                        rmse(ytrain_elnet$Age, ytrain_elnet$PreAge), 
                        rmse(ytrain_lasso$Age, ytrain_lasso$PreAge)),
                 MAE=c(mae(ytrain_ridge$Age, ytrain_ridge$PreAge), 
                       mae(ytrain_elnet$Age, ytrain_elnet$PreAge),
                       mae(ytrain_lasso$Age, ytrain_lasso$PreAge)))
df <- cbind("Model"=c("Ridge","ElasitcNet","Lasso"),df)
df[,2:3] <- round(df[,2:3],3)
df_loo <- ggtexttable(df, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", fill = c("#e8f3de", "#d3e8bb"))
           )
)
df_loo  %>% table_cell_font(row =2:4, column = 1:3, size = 10)
```

```{r}
# Calculate RMSE and MAE for the each group for ridge model
result_loo_ridge <- ytrain_ridge %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) 
result_loo_ridge[,2:3] <- round(result_loo_ridge[,2:3],3)
result_loo_ridge <- result_loo_ridge %>% mutate(Alpha=rep(0,8))
```

```{r}
# Calculate RMSE and MAE for the each group for elastic net model
result_loo_elnet <- ytrain_elnet %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) 
result_loo_elnet[,2:3] <- round(result_loo_elnet[,2:3],3)
result_loo_elnet <- result_loo_elnet %>% mutate(Alpha=rep(0.5,8))
```

```{r}
# Calculate RMSE and MAE for the each group for lasso model
result_loo_lasso <- ytrain_lasso %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) 
result_loo_lasso[,2:3] <- round(result_loo_lasso[,2:3],3)
result_loo_lasso <- result_loo_lasso %>% mutate(Alpha=rep(1,8))
```

--------------------------------------------
Find out models with the lowest RMSE and AME
--------------------------------------------

```{r}
# Combine the result of 3 models 
df2 <- rbind(result_loo_ridge,result_loo_elnet,result_loo_lasso)
# Pick out the lowest value and its corresponding model
RMSE <- df2 %>% group_by(GEO) %>% slice(which.min(RMSE)) %>% select(-MAE) %>% rename(Alpha.RMSE=Alpha)
MAE <- df2 %>% group_by(GEO) %>% slice(which.min(MAE)) %>% select(-RMSE) %>% rename(Alpha.MAE=Alpha)
df2 <- left_join(RMSE, MAE, by='GEO') %>% rename(Cohort=GEO)
df2_loo <- ggtexttable(df2, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", fill = c("#e8f3de", "#d3e8bb"))
           )
)
df2_loo  %>% table_cell_font(row =2:tab_nrow(df2_loo ), column = 1:tab_ncol(df2_loo ), size = 10) 
```

---------------------------------
Build a comparable baseline model
---------------------------------

```{r}
# Store ytrain for baseline models
ytrain_loo <- ytrain
# Apply leave one out for elastic net models 
for (i in unique(ytrain_loo$GEO)){
  onenum <- which(ytrain_loo$GEO==i)
  restnum <- which(ytrain_loo$GEO!=i)
  # Find the best lambda of alpha value with 0.5 for every group
  cv  <- cv.glmnet(xtrain[restnum,], ytrain_loo[restnum,"Age"], family="gaussian", alpha = 0.5, nfolds=10)
  # Fit data with its attached lambda
  fit_loo <- glmnet(xtrain[restnum,], ytrain_loo[restnum,"Age"], family = "gaussian", alpha = 0.5,lambda = cv$lambda.min)
  # Store the predicted ages
  ytrain_loo[onenum,"PreAge"] <- predict(fit_loo, newx = xtrain[onenum,], type = "response", s = "lambda.min")
}
```


```{r}
# Calculate RMSE and MAE for baseline models
result_loo <- ytrain_loo %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) 
result_loo[nrow(result_loo)+1,] <- list("Total", rmse(ytrain_loo$Age, ytrain_loo$PreAge),mae(ytrain_loo$Age,ytrain_loo$PreAge))
result_loo[,2:3] <- round(result_loo[,2:3],3)

tab_loo <- ggtexttable(result_loo, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
 )
)
tab_loo %>% table_cell_font(row =2:tab_nrow(tab_loo), column = 1:tab_ncol(tab_loo), size = 10)  
```

-----------------------------------------------------
Adjust alpha and lambda values to find the best model
-----------------------------------------------------

```{r}
# Store ytrain for parameter tuning models
ytrain_alpha <- ytrain
# Build blank data frame to record alpha and lambda value 
alpha <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(alpha) <- unique(ytrain_alpha$GEO)
colnames(alpha) <- "Alpha"
lambda <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(lambda) <- unique(ytrain_alpha$GEO)
colnames(lambda) <- "Lambda"
# Build blank data frame to record and the number of selected coefficients
coefnum <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(coefnum) <- unique(ytrain_alpha$GEO)
colnames(coefnum) <- "CpG Number"

# Apply parameter tuning and leave one out for elastic net models 
for (i in unique(ytrain_alpha$GEO)){
  onenum <- which(ytrain_alpha$GEO==i)
  restnum <- which(ytrain_alpha$GEO!=i)
  m=50 # set a quite large value for rmse
  for (j in seq(0,1,0.1)) {
    # Tune alpha with the attached lambda by cross validation
    cv  <- cv.glmnet(xtrain[restnum,], ytrain_alpha[restnum,"Age"], family="gaussian", alpha = j, nfolds=10)
    # Fit data with alpha and its attached lambda
    fit_alpha <- glmnet(xtrain[restnum,], ytrain_alpha[restnum,"Age"], family = "gaussian", alpha = j,lambda = cv$lambda.min)
    # Calculate rmse of test data 
    n <- rmse(ytrain$Age[onenum], predict(fit_alpha, newx = xtrain[onenum,],s = "lambda.min"))
    if (n < m){
      m <- n # Only keep the models with lower rmse
      # Store the predicted ages
      ytrain_alpha[onenum,"PreAge"] <- predict(fit_alpha, newx = xtrain[onenum,], type = "response", s = "lambda.min") 
      # Store the corresponding information of the model with the lowest rmse
      alpha[i,1] <- j
      lambda[i,1] <- cv$lambda.min
      coefs <- coef(fit_alpha)
      # Exclude intercept
      coefnum[i,1] <- length(coefs[which(coefs!=0),])-1
    }
  }
}
```


```{r}
# Calculate RMSE and MAE for each group after parameter tuning
result_alpha <- ytrain_alpha %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) %>% mutate(Alpha=as.character(alpha[,1]),Lambda=as.character(round(lambda[,1],3))) %>% rename(Cohort=GEO) 
result_alpha[,"CpG Number"] <- as.character(coefnum$`CpG Number`)
result_alpha[nrow(result_alpha)+1,1:6 ] <- list("Total", rmse(ytrain_alpha$Age, ytrain_alpha$PreAge),mae(ytrain_alpha$Age,ytrain_alpha$PreAge),"-","-", "-")
result_alpha[,2:3] <- round(result_alpha[,2:3],3)

tab_alpha <- ggtexttable(result_alpha, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
 )
)
tab_alpha %>% table_cell_font(row =2:tab_nrow(tab_alpha), column = 1:tab_ncol(tab_alpha), size = 10)  
```

---------------------------------
Combine PCA with parameter tuning
---------------------------------

```{r}
# Store ytrain for pca.tune models
ytrain_pca <- ytrain
# Build blank data frame to record alpha and lambda values 
ytrain_pca_alpha <- ytrain
alpha.pca_alpha <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(alpha.pca_alpha) <- unique(ytrain_pca_alpha$GEO)
colnames(alpha.pca_alpha) <- "Alpha"
lambda.pca_alpha <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(lambda.pca_alpha) <- unique(ytrain_pca_alpha$GEO)
colnames(lambda.pca_alpha) <- "Lambda"
# Build blank data frame to record principal numbers and CpG numbers
pca_alphanum <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(pca_alphanum) <- unique(ytrain_pca_alpha$GEO)
colnames(pca_alphanum) <- "PCs"
coefnum.pca_alpha <- data.frame(matrix(nrow = 8,ncol = 1))
rownames(coefnum.pca_alpha) <- unique(ytrain_pca_alpha$GEO)
colnames(coefnum.pca_alpha) <- "Num"

for (i in unique(ytrain_pca_alpha$GEO)){
  onenum <- which(ytrain_pca_alpha$GEO==i)
  restnum <- which(ytrain_pca_alpha$GEO!=i)
  m=50
  # Implement PCA without center and scale
  pca <- prcomp(xtrain[restnum,])
  # Select principal components according to cumulative variance 
  cum <- summary(pca)$importance[3,]
  num <- length(which(cum<=0.9))
  # Store the number of PC when cumulative variance > 90%
  pca_alphanum[i,1] <- num
  # Keep the PCs with cumulative variance > 90%
  xtrain_pcs <- pca$x[,1:num]
  # Transform test data into the form of principal components
  xtest_pca_alpha <- predict(pca, newdata=xtrain[onenum,])[,1:num]
  for (j in seq(0,1,0.1)) {
    # Train elastic net models with these PCs and tune parameters
    cv_pca_alpha  <- cv.glmnet(xtrain_pcs, ytrain_pca_alpha[restnum,"Age"],
                               family="gaussian", alpha = j, nfolds=10)
    fit_pca_alpha  <- glmnet(xtrain_pcs, ytrain_pca_alpha[restnum,"Age"], 
                             family = "gaussian",alpha = j, 
                             lambda = cv_pca_alpha$lambda.min)
    # Calculate rmse of test data 
    ytrain_pca_alpha.predict <- predict(fit_pca_alpha, newx = xtest_pca_alpha,
                                        type = "response", s = "lambda.min")
    n <- rmse(ytrain_pca_alpha$Age[onenum], ytrain_pca_alpha.predict)
    if (n < m){
      m <- n
      # Store the predicted ages
      ytrain_pca_alpha[onenum,"PreAge"] <- ytrain_pca_alpha.predict
      # Store the corresponding information of the model with the lowest rmse
      alpha.pca_alpha[i,1] <- j
      lambda.pca_alpha[i,1] <- cv_pca_alpha$lambda.min
      coefs <- coef(fit_pca_alpha)
      # Exclude intercept
      coefnum.pca_alpha[i,1] <- length(coefs[which(coefs!=0),])-1
    }
  }
}
```


```{r}
# Calculate RMSE and MAE for each group after PCA and parameter tuning
result_pca_alpha <- ytrain_pca_alpha %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) %>% mutate(Alpha=as.character(alpha.pca_alpha[,1]),Lambda=as.character(round(lambda.pca_alpha[,1],3)),PCs=pca_alphanum[,1],Num=coefnum.pca_alpha[,1]) %>% rename(Cohort=GEO) %>% ungroup()
result_pca_alpha[nrow(result_pca_alpha)+1,] <- list("Total",
                      rmse(ytrain_pca_alpha$Age, ytrain_pca_alpha$PreAge),
                      mae(ytrain_pca_alpha$Age,ytrain_pca_alpha$PreAge), "-","-",round(mean(pca_alphanum[,1])),round(mean(coefnum.pca_alpha[,1])))
result_pca_alpha[,2:3] <- round(result_pca_alpha[,2:3],3)

tab_pca_alpha <- ggtexttable(result_pca_alpha, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
  )
)
tab_pca_alpha %>% table_cell_font(row =2:tab_nrow(tab_pca_alpha), column = 1:tab_ncol(tab_pca_alpha), size = 10) 
```

--------------------------------------------------------
Compare models with PCA and parameter tuning 
and models only with parameter tuning  to baseline model
--------------------------------------------------------

```{r}
# Restore the result
com_final <- result_loo
# Calculate the change rate
com_final[,"Tune.RMSE.Ratio"] <- scales::percent(round((result_alpha$RMSE-com_final$RMSE)/com_final$RMSE,3))
com_final[,"PCA.Tune.RMSE.Ratio"] <- scales::percent((result_pca_alpha$RMSE-com_final$RMSE)/com_final$RMSE)
com_final[,"Tune.MAE.Ratio"] <- scales::percent(round((result_alpha$MAE-com_final$MAE)/com_final$MAE,3))
com_final[,"PCA.Tune.MAE.Ratio"] <- scales::percent((result_pca_alpha$MAE-com_final$MAE)/com_final$MAE,accuracy =0.1)
com_final <- com_final %>% select(GEO,RMSE,Tune.RMSE.Ratio,PCA.Tune.RMSE.Ratio,MAE,Tune.MAE.Ratio,PCA.Tune.MAE.Ratio)
com_final <- as.data.frame(com_final)
colnames(com_final) <- c("Cohort","LOO","Tune.Ratio","PCA.Ratio","LOO","Tune.Ratio","PCA.Ratio")
com_final  %>% kbl(longtable = T, booktabs = T, align = rep("c",6)) %>% add_header_above(c("-", "RMSE"=3, "MAE" = 3), bold=T, color = "white", background = "#8cc257", font_size = 15) %>% kable_styling(latex_options = "HOLD_position") %>% kable_classic(full_width = FALSE, html_font = "arial") %>%
  row_spec(c(0), bold=T, color = "white", background = "#8cc257",font_size = 15) %>%
  row_spec(c(1,3,5,7,9), color = "black", background = "#d3e8bb",font_size = 14) %>%
  row_spec(c(2,4,6,8), color = "black", background = "#e8f3de",font_size = 14)
```

-------------------------------------------------------------------
Extract best models between models with PCA and parameter tuning 
and models only with parameter tuning
-------------------------------------------------------------------

```{r}
# Combine the result of 2 models 
pca_alpha <- result_pca_alpha[1:8,1:6]
pca_alpha <- pca_alpha %>% mutate(Method=rep("PCA",8))
tune <- result_alpha[1:8,1:5]
tune[,"PCs"] <- rep("-",8)
tune <- tune %>% mutate(Method=rep("Tune",8))
df_com <- rbind(pca_alpha,tune)
# Pick out the lowest RMSE and its corresponding model
RMSE.com <- df_com %>% group_by(Cohort) %>% slice(which.min(RMSE)) %>% select(-MAE)

tab_RMSE.com <- ggtexttable(RMSE.com, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", fill = c("#e8f3de", "#d3e8bb"))
           )
)
tab_RMSE.com %>% table_cell_font(row =2:tab_nrow(tab_RMSE.com), column = 1:tab_ncol(tab_RMSE.com), size = 10) 
```

----------------------------------------------------------------------------
Combine the best models for each sub-models to provide an overall best model
----------------------------------------------------------------------------

```{r}
# Store ytrain for final models
ytrain_final <- ytrain
# Run each model again to keep all of the information
# the best sub models of elastic net without CPA
GSE40279.fit <- glmnet(xtrain[which(ytrain$GEO!="GSE40279"),], ytrain[which(ytrain$GEO!="GSE40279"),"Age"], family = "gaussian", alpha = 0.9,lambda = 0.129)
ytrain_final[which(ytrain$GEO=="GSE40279"),"PreAge"] <- predict(GSE40279.fit, newx = xtrain[which(ytrain$GEO =="GSE40279"),], s=0.129)


GSE41169.fit <- glmnet(xtrain[which(ytrain$GEO!="GSE41169"),], ytrain[which(ytrain$GEO!="GSE41169"),"Age"], family = "gaussian", alpha = 0.7,lambda = 0.155)
ytrain_final[which(ytrain$GEO=="GSE41169"),"PreAge"] <- predict(GSE41169.fit, newx = xtrain[which(ytrain$GEO =="GSE41169"),], s=0.155)


GSE42861.fit <- glmnet(xtrain[which(ytrain$GEO!="GSE42861"),], ytrain[which(ytrain$GEO!="GSE42861"),"Age"], family = "gaussian", alpha = 0.5,lambda = 0.267)
ytrain_final[which(ytrain$GEO=="GSE42861"),"PreAge"] <- predict(GSE42861.fit, newx = xtrain[which(ytrain$GEO =="GSE42861"),], s=0.267)


GSE72773.fit <- glmnet(xtrain[which(ytrain$GEO!="GSE72773"),], ytrain[which(ytrain$GEO!="GSE72773"),"Age"], family = "gaussian", alpha = 0.1,lambda = 1.169)
ytrain_final[which(ytrain$GEO=="GSE72773"),"PreAge"] <- predict(GSE72773.fit, newx = xtrain[which(ytrain$GEO =="GSE72773"),], s=1.169)
```


```{r}
# the best sub models of elastic net afer PCA
GSE53740.pca <- prcomp(xtrain[which(ytrain$GEO!="GSE53740"),])
GSE53740.xtrain_pcs <- GSE53740.pca$x[,1:790]
GSE53740.xtest_pca_alpha <- predict(GSE53740.pca, newdata=xtrain[which(ytrain$GEO=="GSE53740"),])[,1:790]
GSE53740.fit_pca_alpha  <- glmnet(GSE53740.xtrain_pcs,
                          ytrain[which(ytrain$GEO!="GSE53740"),"Age"],
                          family = "gaussian",alpha = 0.1, lambda = 0.568)
ytrain_final[which(ytrain$GEO=="GSE53740"),"PreAge"]<- predict(GSE53740.fit_pca_alpha, newx = GSE53740.xtest_pca_alpha, type = "response", 
                                                                     s = 0.568)


GSE72775.pca <- prcomp(xtrain[which(ytrain$GEO!="GSE72775"),])
GSE72775.xtrain_pcs <- GSE72775.pca$x[,1:739]
GSE72775.xtest_pca_alpha <- predict(GSE72775.pca, newdata=xtrain[which(ytrain$GEO=="GSE72775"),])[,1:739]
GSE72775.fit_pca_alpha  <- glmnet(GSE72775.xtrain_pcs,
                          ytrain[which(ytrain$GEO!="GSE72775"),"Age"],
                          family = "gaussian",alpha = 0, lambda = 1.33)
ytrain_final[which(ytrain$GEO=="GSE72775"),"PreAge"]<- predict(GSE72775.fit_pca_alpha, newx = GSE72775.xtest_pca_alpha, type = "response", 
                                                                     s = 1.33)


GSE72777.pca <- prcomp(xtrain[which(ytrain$GEO!="GSE72777"),])
GSE72777.xtrain_pcs <- GSE72777.pca$x[,1:859]
GSE72777.xtest_pca_alpha <- predict(GSE72777.pca, newdata=xtrain[which(ytrain$GEO=="GSE72777"),])[,1:859]
GSE72777.fit_pca_alpha  <- glmnet(GSE72777.xtrain_pcs,
                          ytrain[which(ytrain$GEO!="GSE72777"),"Age"],
                          family = "gaussian",alpha = 0, lambda = 1.251)
ytrain_final[which(ytrain$GEO=="GSE72777"),"PreAge"]<- predict(GSE72777.fit_pca_alpha, newx = GSE72777.xtest_pca_alpha, type = "response", 
                                                                     s = 1.251)  


GSE78874.pca <- prcomp(xtrain[which(ytrain$GEO!="GSE78874"),])
GSE78874.xtrain_pcs <- GSE78874.pca$x[,1:996]
GSE78874.xtest_pca_alpha <- predict(GSE78874.pca, newdata=xtrain[which(ytrain$GEO=="GSE78874"),])[,1:996]
GSE78874.fit_pca_alpha  <- glmnet(GSE78874.xtrain_pcs,
                          ytrain[which(ytrain$GEO!="GSE78874"),"Age"],
                          family = "gaussian",alpha = 0, lambda = 1.275	)
ytrain_final[which(ytrain$GEO=="GSE78874"),"PreAge"]<- predict(GSE78874.fit_pca_alpha, newx = GSE78874.xtest_pca_alpha, type = "response", 
                                                                     s = 1.275) 	
```

```{r}
# Diagnose our linear models by residual plots
ggplot(ytrain_final, aes(x=Age, y=Age-PreAge)) + geom_point(aes(color=GEO),size=0.8) + facet_wrap(~GEO,ncol = 4) + labs(x="True Age", y="Residual") + theme(legend.position = 'none') + scale_color_brewer(palette = "Set2") + geom_abline(intercept = 0, slope = 0, size = 0.4)
```

```{r}
# Observe the predicted effect by comparing with the true ages
ggplot(ytrain_final, aes(x=Age, y=PreAge)) + geom_point(aes(color=GEO),size=0.8) + scale_color_brewer(palette = "Set3") + labs(x="True Age", y="Predicted Age") + theme(legend.position = 'none') + facet_wrap(~GEO,ncol=4) +  geom_abline(intercept = 0, slope = 1, size = 0.4)
```


```{r}
# Calculate RMSE and MAE for each group
result_final <- ytrain_final %>% group_by(GEO) %>% summarise(RMSE = rmse(Age,PreAge), MAE= mae(Age, PreAge)) 

result_final[nrow(result_final) + 1, ] <- list("Total",rmse(ytrain_final$Age, ytrain_final$PreAge), mae(ytrain_final$Age,ytrain_final$PreAge))
result_final[,2:3]<-round(result_final[,2:3],3)
tab_final <- ggtexttable(result_final, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
 )
)
tab_final %>% table_cell_font(row =2:tab_nrow(tab_final), column = 1:tab_ncol(tab_final), size = 10)  
```

```{r}
# Extract the common CpG sites as basic epigenetic clock
GSE40279.coefs <- coef(GSE40279.fit)
GSE40279.coefs <- as.data.frame(GSE40279.coefs[which(GSE40279.coefs!=0),])
GSE40279 <- rownames(GSE40279.coefs)[-1]

GSE41169.coefs <- coef(GSE41169.fit)
GSE41169.coefs <- as.data.frame(GSE41169.coefs[which(GSE41169.coefs!=0),])
GSE41169 <- rownames(GSE41169.coefs)[-1]


GSE42861.coefs <- coef(GSE42861.fit)
GSE42861.coefs <- as.data.frame(GSE42861.coefs[which(GSE42861.coefs!=0),])
GSE42861 <- rownames(GSE42861.coefs)[-1]

GSE72773.coefs <- coef(GSE72773.fit)
GSE72773.coefs <- as.data.frame(GSE72773.coefs[which(GSE72773.coefs!=0),])
GSE72773 <- rownames(GSE72773.coefs)[-1]

clock <- matrix(Reduce(intersect, list(GSE40279,GSE41169,GSE42861,GSE72773)),ncol=6)
```

-------------------------------------
Discuss the influence of variable sex
-------------------------------------

```{r}
# Combine 2 datasets
total <- cbind(xtrain,ytrain)
# Drop missing rows together
total <- total %>% drop_na()
# Transform string to integer
total$Sex <- ifelse(total$Sex=="M", 1, 0)
# Split into xtrain and ytrain
xtrain_nosex <- as.matrix(total[,1:23758])
ytrain_sex <- total[,23759:23761]
# Add sex into design matrix
xtrain_sex <- as.matrix(cbind(xtrain_nosex,total$Sex))
```


```{r}
# Build blank data frame to record and the coefficient of sex
sex.coef <- data.frame(matrix(nrow = 8,ncol = 3))
rownames(sex.coef) <- unique(ytrain_sex$GEO)
colnames(sex.coef) <- c("Ridge","ElasitcNet","Lasso")

for (i in unique(ytrain_sex$GEO)){
  onenum <- which(ytrain_sex$GEO==i)
  restnum <- which(ytrain_sex$GEO!=i)
  # apply ridge, elastic net and lasso cross-validation
  cv.ridge <- cv.glmnet(xtrain_sex[restnum,], ytrain_sex[restnum,"Age"], family="gaussian", alpha = 0, nfolds=10)
  cv.elnet <- cv.glmnet(xtrain_sex[restnum,], ytrain_sex[restnum,"Age"], family="gaussian", alpha = 0.5, nfolds=10)
  cv.lasso <- cv.glmnet(xtrain_sex[restnum,], ytrain_sex[restnum,"Age"], family="gaussian", alpha = 1, nfolds=10)
  # Record the coefficient of sex for each submodel
  sex.coef[i,1] <- coef(cv.ridge)[23760]
  sex.coef[i,2] <- coef(cv.elnet)[23760]
  sex.coef[i,3] <- coef(cv.lasso)[23760]
}
```


```{r}
sex.coef <- rownames_to_column(sex.coef, var = "Cohort")
sex.coef[,2:4] <- round(sex.coef[,2:4],4)
tab.sex.coef <- ggtexttable(sex.coef, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
 )
)
tab.sex.coef %>% table_cell_font(row =2:tab_nrow(tab.sex.coef), column = 1:tab_ncol(tab.sex.coef), size = 10)  
```

------------------------------------------------
Discuss the influence of different stage of age
------------------------------------------------

```{r}
# Observe the predicted effect by comparing with the true ages
ggplot(data=subset(ytrain_final,GEO!="GSE78874"), aes(x=Age, y=PreAge)) + geom_point(aes(color=GEO),size=0.8) + labs(x="True Age", y="Predicted Age") + scale_color_brewer(palette = "Set2") + geom_abline(intercept = 0, slope = 1, size = 0.4)
```


```{r}
# Calculate RMSE and MAE for each age group
agegroup <- ytrain_final %>% filter(GEO!="GSE78874") %>% mutate(AgeStage = case_when(Age<=20 ~ '<=20', Age>20 & Age<80 ~ '20-80', Age>=80 ~ '>=80')) %>% group_by(AgeStage) %>% summarise(RMSE = round(rmse(Age,PreAge),3), MAE= round(mae(Age, PreAge),3)) 

tab.agegroup <- ggtexttable(agegroup, rows = NULL, theme = ttheme(
             colnames.style = colnames_style(color = "white", fill = "#8cc257"),
             tbody.style = tbody_style(color = "black", 
                                       fill = c("#e8f3de", "#d3e8bb"))
 )
)
tab.agegroup %>% table_cell_font(row =2:tab_nrow(tab.agegroup), column = 1:tab_ncol(tab.agegroup), size = 10)  
```

